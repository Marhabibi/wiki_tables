{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from keras import Model, Input\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, K, Lambda, Dense, RepeatVector, TimeDistributed, \\\n",
    "    merge, Activation, Flatten, Permute, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.regularizers import l2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import metrics\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.io.json import json_normalize  \n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "# from DataLoader import DataLoader\n",
    "from table import WikiTable\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Compute P/R/F1 from the confusion matrix.\n",
    "'''\n",
    "def evaluation_metrics_report(mat, labels_, method_, epochs=10):\n",
    "    num_classes = len(mat)\n",
    "    \n",
    "    scores = dict()\n",
    "    avg_p = []\n",
    "    avg_r = []\n",
    "    avg_f1 = []\n",
    "    for i in range(0, num_classes):\n",
    "        p = mat[i,i] / float(sum(mat[:,i]))\n",
    "        r = mat[i,i] / float(sum(mat[i,:]))\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "        \n",
    "        scores[i] = (p, r, f1)\n",
    "        avg_p.append(p)\n",
    "        avg_r.append(r)\n",
    "        avg_f1.append(f1)\n",
    "    \n",
    "    outstr = 'Evaluation results for ' + method_ + ' Epochs: ' + str(epochs) + '\\n'\n",
    "    for key in scores:\n",
    "        label = labels_[key]\n",
    "        val_1 = scores[key][0]\n",
    "        val_2 = scores[key][1]\n",
    "        val_3 = scores[key][2]\n",
    "        \n",
    "        outstr += ('%s\\tP=%.3f\\tR=%.3f\\tF1=%.3f\\n' % (label, val_1, val_2, val_3))\n",
    "    avg_p_score = sum(avg_p) / len(avg_p)\n",
    "    avg_r_score = sum(avg_r) / len(avg_r)\n",
    "    avg_f1_score = sum(avg_f1) / len(avg_f1)\n",
    "    outstr += 'AVG\\tAvg-P=%.3f\\tAvg-R=%.3f\\tAvg-F1=%.3f\\n' % (avg_p_score, avg_r_score, avg_f1_score)\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM column-by-column with attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.batch_dot(Y, alpha)\n",
    "    return ans\n",
    "\n",
    "\n",
    "'''\n",
    "    LSTM baseline where the columns are represented by their title, and LCA category.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_lstm_baseline_w2v(w2v_size, w2v_dim, word2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(input=main_input, output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "    LSTM baseline where the columns are represented by their title, and LCA category.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_lstm_baseline_w2v_lca(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    \n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(input=[main_input, subject_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "    LSTM baseline where the columns are represented by their title, and LCA category.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_lstm_baseline_w2v_lca_val(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    \n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "    \n",
    "        # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    lstm = LSTM(lstm_units, return_sequences=True)(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(lstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(input=[main_input, subject_input, value_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "    BiLSTM baseline where the columns are represented by their title, and LCA category.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_baseline_w2v_lca(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    \n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2], mode='sum')\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(input=[main_input, subject_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    BiLSTM baseline where the columns are represented by their title.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_baseline_w2v(w2v_size, w2v_dim, word2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    drop_out = Dropout(0.3, name='dropout')(e)\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=main_input, output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "    BiLSTM baseline where the columns are represented by their title, LCA category, and column values.\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_baseline_w2v_lca_val(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix, lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.2)(bilstm)\n",
    "    flatten = Flatten()(drop_out)\n",
    "    out = Dense(3, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(input=[main_input, subject_input, value_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "    We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "    The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "    and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "    as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "\n",
    "    The model is based on the paper by Rockta\\\"schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "    \n",
    "    @Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_col_subject_val_model(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix,\n",
    "                                   lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = LEN\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(subject_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    value_input = Input(shape=(N,), dtype='int32', name='value_input')\n",
    "    e3 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(value_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e4 = merge([e, e2, e3], mode='sum')\n",
    "    \n",
    "    drop_out = Dropout(0.3, name='dropout')(e4)\n",
    "    \n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(k, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('tanh', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input, subject_input, value_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "    We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "    The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "    and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "    as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "\n",
    "    The model is based on the paper by Rockta\\\"schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "    \n",
    "        @Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_col_subject_model(w2v_size, w2v_dim, word2vec_matrix, n2v_size, n2v_dim, node2vec_matrix,\n",
    "                                   lstm_units=100, col_length=10):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(w2v_size, w2v_dim, weights=[word2vec_matrix], input_length=N, trainable=False)(main_input)\n",
    "\n",
    "    # add the second input layer which corresponds to the category embeddings\n",
    "    subject_input = Input(shape=(N,), dtype='int32', name='subject_input')\n",
    "    e2 = Embedding(n2v_size, n2v_dim, weights=[node2vec_matrix], input_length=N, trainable=False)(\n",
    "        subject_input)\n",
    "\n",
    "    # merge the two embedding layers\n",
    "    e3 = merge([e, e2], mode='sum')\n",
    "    \n",
    "    drop_out = Dropout(0.3, name='dropout')(e3)\n",
    "    \n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(k, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('tanh', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input, subject_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "    We build a BiLSTM model with attention, which matches two tables based on their column representation.\n",
    "    The attention mechanism decides which columns have their highest match from the second candidate table\n",
    "    and then uses this to compute the attention weights which later on are used for classifying the pair\n",
    "    as either: 'equivalent', 'subPartOf' or 'notAligned'\n",
    "    \n",
    "    The model is based on the paper by Rockta\\\"schel et al. \"Reasoning about Entailment with Neural Attention\"\n",
    "    \n",
    "    @Credit: This is an adaptation of the implementation of the original paper by https://github.com/shyamupa/snli-entailment\n",
    "'''\n",
    "\n",
    "\n",
    "def build_bilstm_col_model(vocab_size, w2v_dim, embedding_matrix, lstm_units=100, col_length=20):\n",
    "    k = 2 * lstm_units\n",
    "    N = col_length\n",
    "    # the first layer is the embeddings for the column names\n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input')\n",
    "    e = Embedding(vocab_size, w2v_dim, weights=[embedding_matrix], input_length=N, trainable=False)(main_input)\n",
    "    drop_out = Dropout(0.1, name='droput')(e)\n",
    "\n",
    "    # pass the input through a BiLSTM model through the previous layer\n",
    "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(drop_out)\n",
    "\n",
    "    # add another drop-out layer\n",
    "    drop_out = Dropout(0.1)(bilstm)\n",
    "\n",
    "    # here we add a custom layer which connects the last input of the first table as the input of the second table\n",
    "    t2 = Lambda(get_H_n, output_shape=(k,), name='h_n')(drop_out)\n",
    "\n",
    "    # add the layer which encodes the output labels\n",
    "    Y = Lambda(get_Y, arguments={'xmaxlen': col_length}, name='Y', output_shape=(col_length, k))(drop_out)\n",
    "    # add the layer which encodes the weight parameter from the LSTMs cells and the hidden states\n",
    "    Whn = Dense(2 * lstm_units, W_regularizer=l2(0.01), name=\"Wh_n\")(t2)\n",
    "    Whn_x_e = RepeatVector(col_length, name=\"Wh_n_x_e\")(Whn)\n",
    "\n",
    "    # add the attention layer\n",
    "    WY = TimeDistributed(Dense(2 * lstm_units, W_regularizer=l2(0.01)), name=\"WY\")(Y)\n",
    "    merged = merge([Whn_x_e, WY], name='merged', mode='sum')\n",
    "    M = Activation('sigmoid', name='M')(merged)\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(1, activation='linear'), name=\"alpha_\")(M)\n",
    "    flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "    alpha = Dense(col_length, activation='softmax', name=\"alpha\")(flat_alpha)\n",
    "\n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,300,20)\n",
    "    r_ = merge([Y_trans, alpha], output_shape=(2 * lstm_units, 1), name=\"r_\", mode=get_R)\n",
    "    r = Reshape((k,), name=\"r\")(r_)\n",
    "\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01))(r)\n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(t2)\n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('sigmoid')(merged)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(h_star)\n",
    "    output = out\n",
    "\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we store the pre-loaded embeddings so that we do not load that every time we do changes in the class.\n",
    "word2vec = None\n",
    "node2vec = None\n",
    "tables = dict()\n",
    "entity_cats = dict()\n",
    "cat_tax = dict()\n",
    "cat_level = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_w2v = None\n",
    "emb_n2v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verse_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from table import WikiTable\n",
    "import re, os, sys\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        # the stop words, which we will use to skip stop words from the column titles\n",
    "        self.stops = set(stopwords.words('english'))\n",
    "        self.word2vec_path = ''\n",
    "        self.node2vec_path = ''\n",
    "        self.cat_taxonomy_path = ''\n",
    "        self.entity_category_path = ''\n",
    "        self.table_data_path = ''\n",
    "        self.table_data_labels = ''\n",
    "        self.out_dir = ''\n",
    "\n",
    "        # data structures that which will hold the necessary data for training the models\n",
    "        self.entity_cats = dict()\n",
    "        self.word2vec = None\n",
    "        self.node2vec = None\n",
    "        self.vocab_n2v = dict()\n",
    "        self.vocab_w2v = dict()\n",
    "        self.DELIM = 0\n",
    "        self.table_pairs = []\n",
    "        self.tables = dict()\n",
    "        self.table_rep = dict()\n",
    "        self.eval_pairs = []\n",
    "        self.cat_tax = dict()\n",
    "        self.cat_parents = dict()\n",
    "        self.cat_level = dict()\n",
    "\n",
    "\n",
    "    '''\n",
    "        Load the word2vec and the node2vec embeddings.\n",
    "    '''\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        self.word2vec = KeyedVectors.load_word2vec_format(self.word2vec_path, binary=False)\n",
    "        self.node2vec = KeyedVectors.load_word2vec_format(self.node2vec_path, binary=False)\n",
    "\n",
    "    def construct_vocab(self):\n",
    "        self.vocab_w2v = {key: idx + 2 for idx, key in enumerate(self.word2vec.vocab)}\n",
    "\n",
    "        # keep zero for the unknown words\n",
    "        self.vocab_w2v['UNK'] = 0\n",
    "        self.vocab_w2v['COL_TOKEN_SPLIT'] = 1\n",
    "\n",
    "        self.vocab_n2v = {key: idx + 2 for idx, key in enumerate(self.node2vec.vocab)}\n",
    "        self.vocab_n2v['UNK'] = 0\n",
    "        self.vocab_n2v['COL_TOKEN_SPLIT'] = 1\n",
    "\n",
    "        # the delimiter which we use to stitch tables together\n",
    "        self.DELIM_N2V = [len(self.vocab_n2v)]\n",
    "        self.DELIM_W2V = [len(self.vocab_w2v)]\n",
    "\n",
    "    '''\n",
    "        Load the evaluation dataset\n",
    "    '''\n",
    "\n",
    "    def load_evaluation_data(self):\n",
    "        for table_pair in self.table_pairs:\n",
    "            table_pair_dict = dict()\n",
    "            table_pair_dict['col_name'] = []\n",
    "            table_pair_dict['col_values'] = []\n",
    "            table_pair_dict['col_subject'] = []\n",
    "            \n",
    "            table_a = self.table_rep[table_pair[0]]\n",
    "            table_b = self.table_rep[table_pair[1]]\n",
    "\n",
    "            # generate the different representations\n",
    "            label = table_pair[2]\n",
    "            \n",
    "            # concatenate the features for the different tables\n",
    "            ep_col_name = np.concatenate((np.concatenate(table_a['col_name'].values()), self.DELIM_W2V, np.concatenate(table_b['col_name'].values())), axis=0)\n",
    "            ep_col_values = np.concatenate((np.concatenate(table_a['col_values'].values()), self.DELIM_N2V, np.concatenate(table_b['col_values'].values())), axis=0)\n",
    "            ep_col_subject = np.concatenate((np.concatenate(table_a['col_subject'].values()), self.DELIM_N2V, np.concatenate(table_b['col_subject'].values())), axis=0)\n",
    "\n",
    "            # ep_col_name = np.concatenate((np.concatenate(table_a['col_name'].values(), axis=0), self.DELIM_W2V, np.concatenate(table_b['col_name'].values(), axis=0)), axis=0)\n",
    "            # ep_col_values = np.concatenate((np.concatenate(table_a['col_values'].values(), axis=0), self.DELIM_N2V, np.concatenate(table_b['col_values'].values(), axis=0)), axis=0)\n",
    "            # ep_col_subject = np.concatenate((np.concatenate(table_a['col_subject'].values(), axis=0), self.DELIM_N2V, np.concatenate(table_b['col_subject'].values(), axis=0)), axis=0)\n",
    "\n",
    "            table_pair_dict['col_name'].append(ep_col_name)\n",
    "            table_pair_dict['col_values'].append(ep_col_values)\n",
    "            table_pair_dict['col_subject'].append(ep_col_subject)\n",
    "\n",
    "            table_pair_dict['table_a'] = table_pair[0]\n",
    "            table_pair_dict['table_b'] = table_pair[1]\n",
    "            table_pair_dict['label'] = label\n",
    "\n",
    "            self.eval_pairs.append(table_pair_dict)\n",
    "\n",
    "    '''\n",
    "        Load the table pairs for alignment with their table data values and their corresponding label\n",
    "    '''\n",
    "\n",
    "    def load_alignment_pairs(self):\n",
    "        # load the table data only for these entities and the table alignment pairs\n",
    "        self.table_pairs = []\n",
    "        \n",
    "        eval_data = DataFrame.from_csv(self.table_data_labels, sep='\\t', index_col=None)\n",
    "        for row_idx, row in eval_data.iterrows():\n",
    "            # the label for the table alignment pair\n",
    "            label = row['label']\n",
    "            # add the entities in the entity index\n",
    "\n",
    "            # add the table pairs as a tuple with the label\n",
    "            tbl_id_a = int(row['tbl_id_a'])\n",
    "            tbl_id_b = int(row['tbl_id_b'])\n",
    "            self.table_pairs.append((tbl_id_a, tbl_id_b, label))\n",
    "            \n",
    "        # load the table data\n",
    "        self.load_tables()\n",
    "\n",
    "    '''\n",
    "        Load the table data for a set of entities of interest.\n",
    "    '''\n",
    "\n",
    "    def load_tables(self):\n",
    "        # read the table data and take only the tables for the entities in the entity index.\n",
    "        fin = gzip.open(self.table_data_path, 'rt')\n",
    "\n",
    "        # return the tables as a dict with the table id as an index.\n",
    "        self.tables = dict()\n",
    "\n",
    "        for line in fin:\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            tbl_json = json.loads(line)\n",
    "            entity = tbl_json['entity']\n",
    "            sections = tbl_json['sections']\n",
    "            for section in sections:\n",
    "                tables_json = section['tables']\n",
    "\n",
    "                for table in tables_json:\n",
    "                    tbl = WikiTable()\n",
    "                    tbl.load_json(json.dumps(table), entity, section, int(table['id']), col_meta_parse=True)\n",
    "                    tbl.markup = table['markup']\n",
    "                    self.tables[tbl.table_id] = tbl\n",
    "\n",
    "    '''\n",
    "        Constructs the evaluation data which we will use to train our DL model. \n",
    "        In this case we will represent the data in the following ways:\n",
    "        \n",
    "            1)  The simplest form of the data representation is in terms of the column names.\n",
    "            2)  We augment the representation with the entities or values present in a column, \n",
    "                in case the values do not link to entities or are not textual, \n",
    "                then we will represent the column data with a UNK vector.\n",
    "            3)  Finally, in the case where (2) reflects entities, we will additionally represent\n",
    "                the data with the corresponding column label (i.e., the LCA category of entities)\n",
    "                \n",
    "    '''\n",
    "\n",
    "    def construct_eval_data(self):\n",
    "        self.table_rep.clear()\n",
    "        # generate the appropriate table representation that we can use for the deep learning models.\n",
    "        for table_id in self.tables:\n",
    "            table = self.tables[table_id]\n",
    "            sub_table_rep = dict()\n",
    "\n",
    "            # generate the column representation, for the columns for which we do not have a word we assign UNK\n",
    "            col_names_rep = dict()\n",
    "            col_val_rep = dict()\n",
    "            col_subj_rep = dict()\n",
    "\n",
    "            for col_idx, column in enumerate(table.column_meta_data):\n",
    "                col_subj_rep[col_idx] = []\n",
    "                col_val_rep[col_idx] = []\n",
    "                col_names_rep[col_idx] = []\n",
    "                \n",
    "                # generate the word embedding for the column name\n",
    "                col_names_rep[col_idx] = self.column_title_to_idx(column, self.vocab_w2v)\n",
    "\n",
    "                # generate the column representation based on the entities\n",
    "                col_values = table.column_meta_data[column]\n",
    "\n",
    "                # take the subset of values which exist in our entity-category index\n",
    "                sub_vals = []\n",
    "                for val in col_values:\n",
    "                    val_label = re.sub(' ', '_', val)\n",
    "                    if val_label in self.vocab_n2v:\n",
    "                        sub_vals.append(val_label)\n",
    "                    \n",
    "                for val in sub_vals:\n",
    "                    col_val_rep[col_idx].append(self.vocab_n2v[val])\n",
    "                col_val_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "\n",
    "                # get the lca category for the values in this column (in case they represent entities)\n",
    "                if len(col_values) != 0:\n",
    "                    lca_cats = self.find_lca_category(col_values)\n",
    "\n",
    "                    if lca_cats is not None:\n",
    "                        for cat in lca_cats: \n",
    "                            cat = 'Category:' + re.sub(' ', '_', cat)\n",
    "                            if cat not in self.vocab_n2v:\n",
    "                                continue\n",
    "                            col_subj_rep[col_idx].append(self.vocab_n2v[cat])\n",
    "                    else:\n",
    "                        col_subj_rep[col_idx].append(0)\n",
    "                \n",
    "                #distinguish between the column representations\n",
    "                col_subj_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "                col_names_rep[col_idx].append(self.vocab_w2v['COL_TOKEN_SPLIT'])\n",
    "                col_val_rep[col_idx].append(self.vocab_n2v['COL_TOKEN_SPLIT'])\n",
    "                \n",
    "            sub_table_rep['col_name'] = col_names_rep\n",
    "            sub_table_rep['col_values'] = col_val_rep\n",
    "            sub_table_rep['col_subject'] = col_subj_rep\n",
    "            self.table_rep[table_id] = sub_table_rep\n",
    "\n",
    "    '''\n",
    "        For a given set of seed entities return their common LCA category. This is in a way representing the subject \n",
    "        or class of the given entities.\n",
    "    '''\n",
    "\n",
    "    def find_lca_category(self, entities):\n",
    "        for entity in entities:\n",
    "            if entity not in self.entity_cats:\n",
    "                continue\n",
    "            if entity not in self.cat_parents:\n",
    "                self.cat_parents[entity] = []\n",
    "            #load all the categories and the parent categories for an entity\n",
    "            for cat in self.entity_cats[entity]:\n",
    "                self.load_cat_parents(cat, self.cat_parents[entity])\n",
    "\n",
    "        # find the common categories\n",
    "        entity = entities[0]\n",
    "        common_cats = set()\n",
    "        index = 0\n",
    "        for entity in entities:\n",
    "            if entity not in self.cat_parents:\n",
    "                continue\n",
    "            if index == 0:\n",
    "                common_cats = set(self.cat_parents[entity])\n",
    "                index += 1\n",
    "            else:\n",
    "                common_cats.intersection(self.cat_parents[entity])\n",
    "\n",
    "        # get the lowest matching category\n",
    "        if len(common_cats) != 0:\n",
    "            common_cat_level = [self.cat_level[cat] for cat in common_cats if cat in self.cat_level]\n",
    "            if len(common_cat_level) != 0:\n",
    "                max_level = max(common_cat_level)\n",
    "                return [cat for cat in common_cats if self.cat_level[cat] == max_level]\n",
    "\n",
    "        return None\n",
    "\n",
    "    '''\n",
    "        Since a column name might have different words, we split and aggregate the word vectors from the resulting words.\n",
    "    '''\n",
    "\n",
    "    def column_title_to_idx(self, column, vocab):\n",
    "        col_words = column.lower().split()\n",
    "        col_rep = []\n",
    "\n",
    "        if len(col_words) == 1:\n",
    "            if col_words[0] in vocab and col_words[0] not in self.stops:\n",
    "                col_rep.append(vocab[col_words[0]])\n",
    "            else:\n",
    "                col_rep.append(vocab['UNK'])\n",
    "        else:\n",
    "            for col in col_words:\n",
    "                if col in vocab and col not in self.stops:\n",
    "                    col_rep.append(vocab[col])\n",
    "                else:\n",
    "                    col_rep.append(vocab['UNK'])\n",
    "        col_rep.append(vocab['COL_TOKEN_SPLIT'])\n",
    "        return col_rep\n",
    "\n",
    "    '''\n",
    "        Load the parents of a category up to the root.\n",
    "    '''\n",
    "\n",
    "    def load_cat_parents(self, cat, parents):\n",
    "        if cat in self.cat_tax and cat not in parents:\n",
    "            sub_parents = self.cat_tax[cat]\n",
    "            parents.append(cat)\n",
    "            \n",
    "            for parent in sub_parents:\n",
    "                self.load_cat_parents(parent, parents)\n",
    "\n",
    "    '''\n",
    "        Load the category taxonomy where each node has contains its parents\n",
    "    '''\n",
    "\n",
    "    def load_flat_cat_tax(self):\n",
    "        for line in gzip.open(self.cat_taxonomy_path, 'rt'):\n",
    "            data = line.strip().split('\\t')\n",
    "            \n",
    "            parent_cat = data[0]\n",
    "            child_cat = data[2]\n",
    "        \n",
    "            if parent_cat not in self.cat_level:\n",
    "                self.cat_level[parent_cat] = int(data[1])\n",
    "            if child_cat not in self.cat_level:\n",
    "                self.cat_level[child_cat] = int(data[3])\n",
    "\n",
    "            if child_cat not in self.cat_tax:\n",
    "                self.cat_tax[child_cat] = []\n",
    "            self.cat_tax[child_cat].append(parent_cat)\n",
    "\n",
    "    '''\n",
    "        Loads the entity categories.\n",
    "    '''\n",
    "\n",
    "    def load_entity_cats(self):\n",
    "        for line in gzip.open(self.entity_category_path, 'rt'):\n",
    "            data = line.strip().split('\\t')\n",
    "            if len(data) != 2:\n",
    "                continue\n",
    "\n",
    "            if data[0] not in self.entity_cats:\n",
    "                self.entity_cats[data[0]] = []\n",
    "            self.entity_cats[data[0]].append(data[1])\n",
    "\n",
    "    '''\n",
    "        The word2vec embedding matrix.\n",
    "    '''\n",
    "\n",
    "    def get_word2vec_matrix(self, emb_dim):\n",
    "        # This will be the embedding matrix\n",
    "        embeddings = 1 * np.random.randn(len(self.vocab_w2v) + 2, emb_dim)\n",
    "        embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "        # Build the embedding matrix\n",
    "        for word, index in self.vocab_w2v.items():\n",
    "            if word in self.word2vec.vocab:\n",
    "                embeddings[index] = self.word2vec.word_vec(word)[:emb_dim]\n",
    "\n",
    "        # del self.word2vec\n",
    "        return embeddings\n",
    "\n",
    "    '''\n",
    "           The node2vec embedding matrix.\n",
    "    '''\n",
    "\n",
    "    def get_node2vec_matrix(self, emb_dim):\n",
    "        # This will be the embedding matrix\n",
    "        embeddings = 1 * np.random.randn(len(self.vocab_n2v) + 2, emb_dim)\n",
    "        embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "        # Build the embedding matrix\n",
    "        for word, index in self.vocab_n2v.items():\n",
    "            if word in self.node2vec.vocab:\n",
    "                embeddings[index] = self.node2vec.word_vec(word)[:emb_dim]\n",
    "        # del self.node2vec\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can download the data from http://l3s.de/~fetahu/wiki_tables/data/"
    "base_dir = '/home/fetahu/wiki_tables/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the class DataLoader\n",
    "loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader.cat_taxonomy_path = base_dir + 'category_data/flat_cat_taxonomy.tsv.gz'\n",
    "loader.table_data_path = '../data/table_data/html_data/structured_html_table_data_ground_truth.json.gz'\n",
    "loader.word2vec_path = base_dir + 'embeddings/glove.6B.300d.emb.gz'\n",
    "loader.node2vec_path = base_dir + 'embeddings/category_entity_label_node2vec.emb.gz'\n",
    "loader.entity_category_path = base_dir + 'category_data/article_cats_201708.tsv.gz'\n",
    "loader.table_data_labels = '../data/gt_data/table_pair_evaluation_eq_sub_irrel_labels.tsv'\n",
    "loader.out_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded word embeddings with 400000 entries and node2vec embeddings with 3374828 entries'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load first the embeddings\n",
    "word2vec = None\n",
    "node2vec = None\n",
    "if word2vec is None and node2vec is None:\n",
    "    loader.load_embeddings()\n",
    "    word2vec = loader.word2vec\n",
    "    node2vec = loader.node2vec\n",
    "else:\n",
    "    loader.word2vec = word2vec\n",
    "    loader.node2vec = node2vec\n",
    "'Loaded word embeddings with %d entries and node2vec embeddings with %d entries' % (len(loader.word2vec.vocab), len(loader.node2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded the word and entity vocabularies'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the vocabularies\n",
    "loader.construct_vocab()\n",
    "'Loaded the word and entity vocabularies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5882458"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded the entity categories for 5882458 entities'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity_cats = None\n",
    "if entity_cats is None or len(entity_cats) == 0:\n",
    "    loader.load_entity_cats()\n",
    "    entity_cats = loader.entity_cats\n",
    "else:\n",
    "    loader.entity_cats = entity_cats\n",
    "'Loaded the entity categories for %d entities' % (len(loader.entity_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded the category taxonomy with 1324003 entries'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cat_tax is None or len(cat_tax) == 0:\n",
    "    loader.load_flat_cat_tax()\n",
    "    cat_tax = loader.cat_tax\n",
    "    cat_level = loader.cat_level\n",
    "else:\n",
    "    loader.cat_tax = cat_tax\n",
    "    loader.cat_level = cat_level\n",
    "'Loaded the category taxonomy with %d entries' % (len(loader.cat_tax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loaded the table data with 18369 entries'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = None\n",
    "if tables is not None:\n",
    "    loader.tables = tables\n",
    "    loader.load_alignment_pairs()\n",
    "else:\n",
    "    loader.load_alignment_pairs()\n",
    "    tables = loader.tables\n",
    "'Loaded the table data with %d entries' % (len(loader.tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "loader.construct_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Constructed the evaluation data for all tables'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load_evaluation_data()\n",
    "'Constructed the evaluation data for all tables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the matrices for the embeddings\n",
    "emb_w2v = loader.get_word2vec_matrix(256)\n",
    "emb_n2v = loader.get_node2vec_matrix(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "X_subj = []\n",
    "X_val = []\n",
    "Y = []\n",
    "\n",
    "for inst in loader.eval_pairs:\n",
    "    X += inst['col_name']\n",
    "    X_subj += inst['col_subject']\n",
    "    X_val += inst['col_values']\n",
    "    Y += [inst['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "Y_fit = encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['equivalent', 'noalignment', 'subpartof'], dtype='|S11')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEN = max(set([len(x) for x in X]) | set([len(x) for x in X_val]) | set([len(x) for x in X_subj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = pad_sequences(X, maxlen=LEN,value=loader.vocab_w2v['UNK'], padding='pre')\n",
    "X_val_subj = pad_sequences(X_subj, maxlen=LEN,value=loader.vocab_n2v['UNK'], padding='pre')\n",
    "X_val_val = pad_sequences(X_val, maxlen=LEN,value=loader.vocab_n2v['UNK'], padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_val, Y_fit, test_size=0.3, random_state=1)\n",
    "X_train_subj, X_test_subj, y_train_subj, y_test_subj = train_test_split(X_val_subj, Y_fit, test_size=0.3, random_state=1)\n",
    "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_val_val, Y_fit, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model -- Column Title Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tablenet_desc = build_bilstm_col_model(len(emb_w2v), 256, emb_w2v, 50, LEN)\n",
    "tablenet_desc.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_tablenet_val = tablenet_desc.predict([X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_tablenet_val = cm(y_test.argmax(axis=1), y_pred_tablenet_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(mat_tablenet_val, encoder.classes_, 'TableNet - Column', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablenet_desc_lca = build_bilstm_col_subject_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablenet_desc_lca.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_tablenet_desc_lca = tablenet_desc_lca.predict([X_test, X_test_subj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_tablenet_desc_lca = cm(y_test.argmax(axis=1), y_pred_tablenet_desc_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(mat_tablenet_desc_lca, encoder.classes_, 'TableNet - Column+LCA', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column title, LCA and Value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablenet_desc_val_lca = build_bilstm_col_subject_val_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablenet_desc_val_lca.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_tablenet_col_lca_val = tablenet_desc_val_lca.predict([X_test, X_test_subj, X_test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_tablenet_col_lca_val = cm(y_test.argmax(axis=1), y_pred_tablenet_col_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(m_tablenet_col_lca_val, encoder.classes_, 'TableNet - Column+VAL+LCA', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableNet Model - Column title, and value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tablenet_desc_val = build_bilstm_col_subject_model(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablenet_desc_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_tablenet_col_val = tablenet_desc_val.predict([X_test, X_test_val])\n",
    "m4 = cm(y_test.argmax(axis=1), y_pred_tablenet_col_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(m4, encoder.classes_, 'TableNet - Column+VAL', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, LCA, value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_lca = build_bilstm_baseline_w2v_lca_val(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_lca.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_blst_lca_val = bilstm_baseline_lca.predict([X_test, X_test_subj, X_test_val])\n",
    "blstm_lca_scores = cm(y_test.argmax(axis=1), y_pred_blst_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(blstm_lca_scores, encoder.classes_, 'BiLSTM - Column+VAL+LCA', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, VAL representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_val = build_bilstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2117e07210>"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_baseline_w2v_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_b_w2v_val = bilstm_baseline_w2v_val.predict([X_test, X_test_subj])\n",
    "b_w2v_val_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(b_w2v_val_mat, encoder.classes_, 'BiLSTM - Column+VAL', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title, LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v_lca = build_bilstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2120b39110>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_baseline_w2v_lca.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_b_w2v_lca = bilstm_baseline_w2v_lca.predict([X_test, X_test_subj])\n",
    "b_w2v_lca_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(b_w2v_lca_mat, encoder.classes_, 'BiLSTM - Column+LCA', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM baseline - column title representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v = build_bilstm_baseline_w2v(len(emb_w2v), 256, emb_w2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bilstm_baseline_w2v.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_b_w2v = bilstm_baseline_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_w2v_mat = cm(y_test.argmax(axis=1), y_pred_b_w2v.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(b_w2v_mat, encoder.classes_, 'BiLSTM - Column', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, and VAL representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f211f499ed0>"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_w2v_val = build_lstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f211f499e50>"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_w2v_val.fit(x=[X_train, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_val = model_lstm_w2v_val.predict([X_test, X_test_subj])\n",
    "lstm_w2v_val_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(lstm_w2v_val_mat, encoder.classes_, 'LSTM - Column + VAL', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, and LCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lstm_w2v = build_lstm_baseline_w2v_lca(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)\n",
    "model_lstm_w2v.fit(x=[X_train, X_train_subj], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_lca = model_lstm_w2v.predict([X_test, X_test_subj])\n",
    "lstm_w2v_lca_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_lca.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(lstm_w2v_lca_mat, encoder.classes_, 'LSTM - Column + LCA', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline  - column title representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lstm_w2v_col = build_lstm_baseline_w2v(len(emb_w2v), 256, emb_w2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2119338c50>"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm_w2v_col.fit(x=X_train, y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_col = model_lstm_w2v_col.predict(X_test)\n",
    "lstm_w2v_col_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_col.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(lstm_w2v_col_mat, encoder.classes_, 'LSTM - Column', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline - column title, LCA, value representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lstm_w2v_lca_val = build_lstm_baseline_w2v_lca_val(len(emb_w2v), 256, emb_w2v, len(emb_n2v), 256, emb_n2v, 50, LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lstm_w2v_lca_val.fit(x=[X_train, X_train_subj, X_train_val], y=y_train, batch_size=100, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_lstm_w2v_col_lca_val = model_lstm_w2v_lca_val.predict([X_test, X_test_subj, X_test_val])\n",
    "lstm_w2v_col_lca_val_mat = cm(y_test.argmax(axis=1), y_pred_lstm_w2v_col_lca_val.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print evaluation_metrics_report(lstm_w2v_col_lca_val_mat, encoder.classes_, 'LSTM - Column+VAL+LCA', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
